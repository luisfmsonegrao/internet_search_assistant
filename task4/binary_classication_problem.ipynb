{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e291235",
   "metadata": {},
   "source": [
    "Load data. Drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Training_part1.csv\",sep=\";\")\n",
    "df1 = df1.drop_duplicates() #There are 370 duplicated rows in both dataframes (duplicate: all values in row, including 'id', are equal to another row)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18356364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Training_part2.csv\",sep=\";\")\n",
    "df2 = df2.drop_duplicates() #There are 370 duplicated rows in both dataframes (duplicate: all values in row, including 'id', are equal to another row)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b279e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joint = pd.merge(df1, df2, on='id', how='outer')\n",
    "df_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa784ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_joint.drop('id',axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4bc04",
   "metadata": {},
   "source": [
    "Check class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adc3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['Class']=='y') #There's big class imbalance (~300 negatives, ~3400 positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9409d",
   "metadata": {},
   "source": [
    "Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95898605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # Some columns with a few dozen NAN values, RAS has >2000 NAN values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ef124",
   "metadata": {},
   "source": [
    "Test collinearity of FAN and NUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FAN\"].corr(df[\"NUS\"]) #NUS AND FAN looked collinear on visual inspection, check correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"NUS\",axis=1) #FAN AND NUS are collinear. Remove NUS to avoid multicollinearity issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258038a",
   "metadata": {},
   "source": [
    "Test collinearity of ERG and GJAH, RAS and XIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd148dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency = pd.crosstab(df['RAS'], df['XIN']) # ERG AND GJAH looked collinear on visual inspection, check cramers V metric\n",
    "                                                # RAS AND XIN also look collinear, check cramers V\n",
    "                                                # to be fully on the safe side, should have renamed the labels of one variable to match the names of the labels of the other\n",
    "chi2 = chi2_contingency(contingency)[0]\n",
    "n = contingency.sum().sum()\n",
    "phi2 = chi2 / n\n",
    "r, k = contingency.shape\n",
    "cramers_v = np.sqrt(phi2 / min(r-1, k-1))\n",
    "\n",
    "print(cramers_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658cb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"GJAH\",axis=1) # ERG AND GJAH are collinear. Remove GJAH to avoid multicollinearity issues\n",
    "df = df.drop(\"RAS\",axis=1) # RAS and XIN are collinear, and RAS is noisy (there are many missing values), so it's preferable to remove RAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478cb81",
   "metadata": {},
   "source": [
    "Plot class-conditional univariate distributions to check for clear hints of which features are discriminative, which features aren't. (Numerical features)\n",
    "\n",
    "Note: To make this analysis statistically significant I would run ANOVA and Kruskall-Wallis tests for each combination of one feature and the class label, to judge whether there's significant statistical association between the two variables. Plotting the class-conditional univariate distributions is an inexact but faster way to assess this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3fab0",
   "metadata": {},
   "source": [
    "BIB: Class-conditional distributions look considerably different, feature is discriminative. Looks like the numerical feature with most different class-conditional distributions\n",
    "\n",
    "FAN: Class-conditional distributions look somewhat different, feature has some discriminative power\n",
    "\n",
    "SIS: Class-conditional distributions look somewhat different, feature has some discriminative power\n",
    "\n",
    "LUK: class 'y' has very extreme outliers but, apart from those, class-conditional distributions are nearly identical. No significant discriminative power.\n",
    "\n",
    "UIN: class-conditional distributions are nearly identical. No significant discriminative power.\n",
    "\n",
    "WET: class-conditional distributions are nearly identical. No significant discriminative power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_feat = 'Class' #Plotting univariate class-conditional distributions for hints of which numerical features might be discriminative. To make this precise, should run ANOVA/Kruskall-Wallis tests\n",
    "class_value = 'y'\n",
    "df[df[class_feat]==class_value].hist(bins=50) # Note: this doesn't plot non-numerical features \n",
    "#ax.set_xlim(0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647584ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_value = 'n'\n",
    "df[df[class_feat]==class_value].hist(bins=50)\n",
    "#ax.set_xlim(0,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ee4b8",
   "metadata": {},
   "source": [
    "Plot class-conditional univariate distributions to check for clear hints of which features are discriminative, which features aren't. (Categorical features)\n",
    "\n",
    "Note: To make this analysis statistically significant I would run Chi-Squared and Cramer's V tests for each combination of one feature and the class label, to judge whether there's significant statistical association between the two variables. Plotting the class-conditional univariate distributions is an inexact but faster way to assess this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd3170",
   "metadata": {},
   "source": [
    "COD: Class-conditional distributions are identical. No discriminative power.\n",
    "\n",
    "ERG: Class-conditional distributions are different. Feature has some discriminative power.\n",
    "\n",
    "MYR: Some categories of MYR have very different sample frequencies per class. Feature has some discriminative power, would need to convert to on-hot encoded variable, or set of binary variables, to use in a decision tree classifier.\n",
    "\n",
    "PKD: same observations as MYR.\n",
    "\n",
    "TOK: Class-conditional distributions are somewhat different. Feature has some discriminative power.\n",
    "\n",
    "VOL: Class-conditional distributions are very different. Feature has strong discriminative power.\n",
    "\n",
    "XIN: Class-conditional distributions are very different. Feature has strong discriminative power.\n",
    "\n",
    "KAT: Class-conditional distributions are only slightly different. Feature has little discriminative power.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07804b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariate_name = \"KAT\" #VOL, XIN and ERG look like they are discriminative. Chi-Squared/Cramers V tests could confirm this.\n",
    "target=\"y\"\n",
    "bins_temp = df[df[class_feat]==target][covariate_name].unique()\n",
    "bins = np.array([i for i in bins_temp if str(i).lower() != 'nan'])\n",
    "bins.sort()\n",
    "df[df[class_feat]==target][covariate_name].value_counts().loc[bins].plot.bar() #use for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba592a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"n\"\n",
    "bins_temp = df[df[class_feat]==target][covariate_name].unique()\n",
    "bins = np.array([i for i in bins_temp if str(i).lower() != 'nan'])\n",
    "bins.sort()\n",
    "df[df[class_feat]==target][covariate_name].value_counts().loc[bins].plot.bar() #use for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4eddc0",
   "metadata": {},
   "source": [
    "VOL + XIN + ERG already enable quite good class-separation as seen from plots of class-conditional joint distribution of these three variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VOL_XIN_ERG'] = df['VOL'] + '_' + df['XIN'] + '_' + df['ERG'] #Looks like it's possible to achieve quite decent class separation with these three variables. Will throw in the best numerical variable ('BIB') as well.\n",
    "covariate_name = \"VOL_XIN_ERG\"\n",
    "target=\"y\"\n",
    "bins_temp = df[df[class_feat]==target][covariate_name].unique()\n",
    "bins = np.array([i for i in bins_temp if str(i).lower() != 'nan'])\n",
    "bins.sort()\n",
    "df[df[class_feat]==target][covariate_name].value_counts().loc[bins].plot.bar() #use for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74308f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"n\"\n",
    "bins_temp = df[df[class_feat]==target][covariate_name].unique()\n",
    "bins = np.array([i for i in bins_temp if str(i).lower() != 'nan'])\n",
    "bins.sort()\n",
    "df[df[class_feat]==target][covariate_name].value_counts().loc[bins].plot.bar() #use for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6437304",
   "metadata": {},
   "source": [
    "Impute missing ERG values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() #There are 4 samples with missing ERG values, all in 'yes' class, for now will impute the class-conditional mode on those samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca084663",
   "metadata": {},
   "outputs": [],
   "source": [
    "erg_y_mode = df[df[\"Class\"]=='y'][\"ERG\"].mode()[0] #for now will impute the missing ERG values with the class-conditional mode\n",
    "df['ERG'].fillna(erg_y_mode,inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f239df",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [\"BIB\", \"ERG\", \"VOL\", \"XIN\",\"Class\"] #keep only these features to train classification model on\n",
    "df_store = df[column_list]\n",
    "df_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d056ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store.to_csv(\"processed_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd4c88",
   "metadata": {},
   "source": [
    "Dataset is tabular, with both numerical and categorical features. Good class separation seems achievable with model including only small subset of features. Based on this, will use a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5295882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fa056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv(\"processed_data.csv\")\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca38bb",
   "metadata": {},
   "source": [
    "Transform categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a96c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.get_dummies(df_processed,columns = [\"ERG\"],drop_first=False) # One-hot encode multinomial categorical feature\n",
    "df_processed['VOL'] = df_processed['VOL'].map({'t': True, 'f': False})\n",
    "df_processed['XIN'] = df_processed['XIN'].map({'t': True, 'f': False})\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"transformed_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a995be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pd.read_csv(\"transformed_data.csv\")\n",
    "df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_transformed['Class']\n",
    "df_features = df_transformed.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27481ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(df_features, df_target, test_size=0.2, stratify=df_target, random_state=42) #startify ensures class proprtions are preserved in train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b189e5",
   "metadata": {},
   "source": [
    "Fit Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(class_weight='balanced', random_state=42,max_depth=10) #class_weight=balanced weights samples inversely to class proportions to compensate for class-imbalance\n",
    "# max_depth = 10 to avoid overfitting. Given limited amount of features, several of which are binomial categorical features, small depth should be sufficient.\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af68c636",
   "metadata": {},
   "source": [
    "Plot accuracy, F1 and AUC. Since there's high class-imbalance, F1 and AUC metrics are more informative than accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_eval)\n",
    "y_proba = model.predict_proba(X_eval)[:, 1] \n",
    "metrics = {\n",
    "    'Accuracy': accuracy_score(y_eval, y_pred),\n",
    "    'F1': f1_score(y_eval, y_pred,pos_label='y'),\n",
    "    'ROC-AUC': roc_auc_score(y_eval, y_proba)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(metrics.keys(), metrics.values(), color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Decision Tree Evaluation Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffaf949",
   "metadata": {},
   "source": [
    "Plot ROC curve to see how model performance varies with decision threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbin = LabelBinarizer()\n",
    "y_eval_bin = lbin.fit_transform(y_eval)\n",
    "y_proba = model.predict_proba(X_eval)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_eval_bin, y_proba)\n",
    "auc = roc_auc_score(y_eval_bin, y_proba)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "num_points = 5\n",
    "indices = np.linspace(0, len(thresholds)-1, num_points, dtype=int)\n",
    "for i in indices:\n",
    "    plt.text(fpr[i], tpr[i], f'{thresholds[i]:.2f}')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve with Thresholds')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690e3ac",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model,'decision_tree_classifier.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
